# ============================================
# AURA BACKEND CONFIGURATION
# ============================================
# Copy this file to .env and fill in your values

# ============================================
# DATABASE CONFIGURATION (SurrealDB)
# ============================================
SURREAL_URL=ws://localhost:8000/rpc
SURREAL_USER=root
SURREAL_PASS=root
SURREAL_NS=aura
SURREAL_DB=main

# ============================================
# OPENROUTER API CONFIGURATION
# ============================================
# Required for LLM inference and embeddings
# Get your key at: https://openrouter.ai/keys
OPENROUTER_API_KEY=OPENROUTER_API_KEY
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# ============================================
# LLM MODEL CONFIGURATION (Hot-Swappable)
# ============================================
# L1 (Instinct Layer): Fast responses, <500ms
# Recommended: Mistral 7B, Gemma, or other lightweight models
AURA_L1_MODEL=mistralai/mistral-7b-instruct

# L2 (Reasoning Layer): Deep analysis, async processing
# Recommended: Claude 3.5 Sonnet, GPT-4, or DeepSeek-R1
AURA_L2_MODEL=anthropic/claude-3.5-sonnet

# L3 (Synthesis Layer): Primary response generation
# Recommended: DeepSeek Chat, Gemini Pro, Claude 3 Haiku
AURA_L3_MODEL=deepseek/deepseek-chat

# ============================================
# EMBEDDINGS CONFIGURATION (Semantic Search)
# ============================================
# Model for generating vector embeddings (must be 1536 dimensions)
# Recommended: OpenAI text-embedding-3-small (via OpenRouter)
AURA_EMBEDDING_MODEL=openai/text-embedding-3-small
AURA_EMBEDDING_DIMENSION=1536

# ============================================
# APPLICATION CONFIGURATION
# ============================================
ENVIRONMENT=development
LOG_LEVEL=INFO
API_HOST=0.0.0.0
API_PORT=8080

# ============================================
# EMOTION ENGINE CONFIGURATION
# ============================================
# Seconds between emotion physics ticks
EMOTION_TICK_RATE=5.0

# Seconds between emotion state persistence to database
EMOTION_PERSISTENCE_INTERVAL=60.0

# ============================================
# CORS CONFIGURATION
# ============================================
# Comma-separated list of allowed origins
CORS_ORIGINS=http://localhost:3000,http://localhost:3001

# ============================================
# RECOMMENDED MODEL ALTERNATIVES
# ============================================
# Feel free to experiment! Here are tested alternatives:

# --- Budget-Conscious Setup ---
# AURA_L1_MODEL=google/gemma-7b-it
# AURA_L2_MODEL=anthropic/claude-3-haiku
# AURA_L3_MODEL=google/gemini-flash-1.5

# --- Maximum Performance Setup ---
# AURA_L1_MODEL=mistralai/mistral-7b-instruct
# AURA_L2_MODEL=anthropic/claude-3-opus
# AURA_L3_MODEL=anthropic/claude-3.5-sonnet

# --- All-DeepSeek Setup ---
# AURA_L1_MODEL=deepseek/deepseek-chat
# AURA_L2_MODEL=deepseek/deepseek-chat
# AURA_L3_MODEL=deepseek/deepseek-chat

# --- Experimental Local + Cloud Hybrid ---
# AURA_L1_MODEL=local/phi-3-mini-4k  # If running local Ollama
# AURA_L2_MODEL=anthropic/claude-3.5-sonnet
# AURA_L3_MODEL=deepseek/deepseek-chat

# ============================================
# NOTES
# ============================================
# - All model names follow OpenRouter's format: provider/model-name
# - Check available models: https://openrouter.ai/models
# - Pricing varies significantly - monitor your usage!
# - L1 should be fast and cheap (called frequently)
# - L2 can be expensive (runs async in background)
# - L3 should balance speed and quality (primary user interaction)

